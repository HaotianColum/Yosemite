{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AccessiTrip AI: Smart Accessible Travel Planner\n",
    "\n",
    "A 2-hour MVP demo leveraging Databricks, Unity Catalog, Mosaic AI, Delta, RAG, agent tools, MLflow, and Streamlit.  \n",
    "**Purpose:** Help travelers (with or without accessibility needs) find accessible places to stay and visit in any US city, using Bright Initiative datasets (Airbnb, Booking.com, Google Maps Businesses).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Access: Delta Tables & Demo Data\n",
    "\n",
    "This notebook can use either real Delta tables (from Unity Catalog) or in-memory demo data. Toggle the `USE_DEMO_DATA` flag below.\n",
    "\n",
    "- **Delta Table Mode:** Reads from Unity Catalog tables (Airbnb, Booking.com, Google Maps Businesses). See `partner_data_quickstart.ipynb` and `nimble-mcp.ipynb` for patterns.\n",
    "- **Demo Mode:** Uses small in-memory DataFrames for fast prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle this to False to use real Delta tables\n",
    "USE_DEMO_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG: Databricks Vector Search Integration\n",
    "\n",
    "This section demonstrates how to use Databricks Vector Search for retrieval-augmented generation (RAG) over your Delta tables. (See also: vector_search_fm_api.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# Set up vector search client and index names\n",
    "CATALOG = \"main\"\n",
    "DB = \"bright_airbnb\"\n",
    "SOURCE_TABLE_NAME = \"listings\"\n",
    "SOURCE_TABLE_FULLNAME = f\"{CATALOG}.{DB}.{SOURCE_TABLE_NAME}\"\n",
    "VS_ENDPOINT_NAME = \"vs_endpoint\"\n",
    "VS_INDEX_NAME = \"accessitrip_vs_index\"\n",
    "VS_INDEX_FULLNAME = f\"{CATALOG}.{DB}.{VS_INDEX_NAME}\"\n",
    "\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "# Ensure endpoint exists\n",
    "if vsc.list_endpoints().get('endpoints') is None or not VS_ENDPOINT_NAME in [e.get('name') for e in vsc.list_endpoints().get('endpoints')]:\n",
    "    vsc.create_endpoint(VS_ENDPOINT_NAME)\n",
    "vsc.wait_for_endpoint(VS_ENDPOINT_NAME, 600)\n",
    "\n",
    "# Ensure index exists\n",
    "if not VS_INDEX_FULLNAME in [i.get(\"name\") for i in vsc.list_indexes(VS_ENDPOINT_NAME).get('vector_indexes', [])]:\n",
    "    vsc.create_delta_sync_index_and_wait(\n",
    "        endpoint_name=VS_ENDPOINT_NAME,\n",
    "        index_name=VS_INDEX_FULLNAME,\n",
    "        source_table_name=SOURCE_TABLE_FULLNAME,\n",
    "        pipeline_type=\"TRIGGERED\",\n",
    "        primary_key=\"id\",\n",
    "        embedding_source_column=\"description\",\n",
    "        embedding_model_endpoint_name=\"databricks-bge-large-en\"\n",
    "    )\n",
    "\n",
    "index = vsc.get_index(endpoint_name=VS_ENDPOINT_NAME, index_name=VS_INDEX_FULLNAME)\n",
    "\n",
    "# Example RAG query\n",
    "rag_results = index.similarity_search(\n",
    "    columns=[\"name\", \"description\", \"amenities\"],\n",
    "    query_text=\"wheelchair accessible hotel in Chicago\",\n",
    "    num_results=3\n",
    ")\n",
    "\n",
    "# Convert results to DataFrame for downstream use\n",
    "import pandas as pd\n",
    "if rag_results and rag_results.get('result') and rag_results['result'].get('data_array'):\n",
    "    rag_df = pd.DataFrame(rag_results['result']['data_array'], columns=[\"name\", \"description\", \"amenities\"])\n",
    "else:\n",
    "    rag_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modular Agent Tools: SQL, RAG, LLM, Logging\n",
    "\n",
    "This section defines agent tools for:\n",
    "- Lodging/business search (SQL or RAG)\n",
    "- Accessibility review summarization (LLM)\n",
    "- Message drafting (LLM)\n",
    "- MLflow logging\n",
    "\n",
    "The agent can use either SQL (Delta tables), RAG (vector search), or demo data as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_databricks import ChatDatabricks\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import os\n",
    "\n",
    "w = WorkspaceClient()\n",
    "os.environ[\"DATABRICKS_HOST\"] = w.config.host\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = w.tokens.create(comment=\"for model serving\", lifetime_seconds=1200).token_value\n",
    "llm = ChatDatabricks(endpoint=\"databricks-llama-4-maverick\", max_tokens=1024)\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "def find_accessible_lodging(city, need=None, top_n=3, use_rag=False):\n",
    "    if use_rag:\n",
    "        # Use RAG/vector search\n",
    "        query = f\"{need or ''} hotel in {city}\"\n",
    "        rag_results = index.similarity_search(\n",
    "            columns=[\"name\", \"description\", \"amenities\"],\n",
    "            query_text=query,\n",
    "            num_results=top_n\n",
    "        )\n",
    "        if rag_results and rag_results.get('result') and rag_results['result'].get('data_array'):\n",
    "            df = pd.DataFrame(rag_results['result']['data_array'], columns=[\"name\", \"description\", \"amenities\"])\n",
    "            df[\"price\"] = None  # Add price if available in your index\n",
    "            df[\"reviews\"] = \"[]\"  # Add reviews if available\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    else:\n",
    "        # Use demo or SQL data\n",
    "        df = lodging_df[lodging_df[\"location\"].str.contains(city, case=False)]\n",
    "        if need:\n",
    "            mask = (\n",
    "                df[\"amenities\"].str.contains(need, case=False, na=False) |\n",
    "                df[\"description\"].str.contains(need, case=False, na=False) |\n",
    "                df[\"reviews\"].apply(lambda reviews: any(need.lower() in r.lower() for r in reviews))\n",
    "            )\n",
    "            df = df[mask]\n",
    "        return df.head(top_n)\n",
    "\n",
    "def summarize_reviews_llm(reviews, need=None):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        You are an expert travel assistant. Summarize the following user reviews for accessibility features{need_clause}.\n",
    "        Reviews:\n",
    "        {reviews}\n",
    "        \"\"\".strip()\n",
    "    )\n",
    "    need_clause = f\" (focus on '{need}')\" if need else \"\"\n",
    "    context = {\"reviews\": \"\\n\".join(reviews), \"need_clause\": need_clause}\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    return chain.invoke(context)\n",
    "\n",
    "def find_accessible_businesses(city, need=None, top_n=2):\n",
    "    df = business_df[business_df[\"location\"].str.contains(city, case=False)]\n",
    "    if need:\n",
    "        mask = (\n",
    "            df[\"accessibility\"].str.contains(need, case=False, na=False) |\n",
    "            df[\"reviews\"].apply(lambda reviews: any(need.lower() in r.lower() for r in reviews))\n",
    "        )\n",
    "        df = df[mask]\n",
    "    return df.sort_values(\"rating\", ascending=False).head(top_n)\n",
    "\n",
    "def draft_inquiry_message(place_name, need):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Draft a polite message to {place_name} asking to confirm if they have {need} available for guests.\n",
    "        \"\"\".strip()\n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    return chain.invoke({\"place_name\": place_name, \"need\": need})\n",
    "\n",
    "def log_agent_action(user_query, params, results):\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_text(str(results), \"results.txt\")\n",
    "    mlflow.log_text(user_query, \"user_query.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streamlit UI: Unified Experience\n",
    "\n",
    "A simple UI for city/needs input, results display, and agent Q&A. Toggle between RAG and SQL/demo search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(page_title=\"AccessiTrip AI\", layout=\"wide\")\n",
    "st.title(\"AccessiTrip AI: Smart Accessible Travel Planner\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"Plan Your Accessible Trip\")\n",
    "    city = st.text_input(\"Where are you traveling?\", value=\"Chicago\")\n",
    "    need = st.text_input(\"Any accessibility or preference needs? (e.g. wheelchair, pet, quiet)\", value=\"wheelchair\")\n",
    "    use_rag = st.checkbox(\"Use RAG/Vector Search\", value=False)\n",
    "    if st.button(\"Search\"):\n",
    "        st.session_state[\"search\"] = True\n",
    "\n",
    "if st.session_state.get(\"search\"):\n",
    "    # Lodging results\n",
    "    st.subheader(\"Accessible Stays\")\n",
    "    stays = find_accessible_lodging(city, need, use_rag=use_rag)\n",
    "    for _, row in stays.iterrows():\n",
    "        st.markdown(f\"**{row.get('name','')}** â€” ${row.get('price','N/A')} per night\")\n",
    "        st.markdown(f\"*Amenities*: {row.get('amenities','')}\")\n",
    "        summary = summarize_reviews_llm(row.get(\"reviews\", []), need) if row.get(\"reviews\") else \"\"\n",
    "        st.markdown(f\"*Why we suggest this*: {summary}\")\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "    # Businesses\n",
    "    st.subheader(\"Nearby Accessible Restaurants/Attractions\")\n",
    "    businesses = find_accessible_businesses(city, need)\n",
    "    for _, row in businesses.iterrows():\n",
    "        st.markdown(f\"**{row['name']}** ({row['type']}, {row['rating']}â­)\")\n",
    "        st.markdown(f\"*Accessibility*: {row['accessibility']}\")\n",
    "        summary = summarize_reviews_llm(row[\"reviews\"], need)\n",
    "        st.markdown(f\"*Review summary*: {summary}\")\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "    # Suggested inquiry message\n",
    "    st.subheader(\"Copy Suggested Inquiry Email/Message\")\n",
    "    if not stays.empty:\n",
    "        msg = draft_inquiry_message(stays.iloc[0][\"name\"], need)\n",
    "        st.code(msg, language=\"text\")\n",
    "\n",
    "    # Log actions\n",
    "    log_agent_action(\n",
    "        user_query=f\"city={city}, need={need}, use_rag={use_rag}\",\n",
    "        params={\"city\": city, \"need\": need, \"use_rag\": use_rag},\n",
    "        results={\"stays\": stays.to_dict(), \"businesses\": businesses.to_dict()}\n",
    "    )\n",
    "\n",
    "    # Agent Q&A\n",
    "    st.subheader(\"Not sure? Ask the Agent!\")\n",
    "    user_q = st.text_input(\"Ask a specific question (e.g. 'Is there an accessible bathroom?')\")\n",
    "    if st.button(\"Ask Agent\"):\n",
    "        all_reviews = []\n",
    "        for _, row in stays.iterrows():\n",
    "            if row.get(\"reviews\"):\n",
    "                all_reviews.extend(row[\"reviews\"])\n",
    "        answer = summarize_reviews_llm(all_reviews, user_q)\n",
    "        st.markdown(f\"**Agent:** {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Extensibility\n",
    "\n",
    "- Each section is a modular agent tool/function.\n",
    "- Swap in real Delta/Unity tables for production.\n",
    "- Add more datasets (insurance, maps, etc.) as needed.\n",
    "- MLflow logs all actions for traceability.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Demo Output Example\n",
    "\n",
    "> **Top Accessible Hotels:**  \n",
    "> - [The Grand Loop Hotel]: â€œStep-free entrance, elevator, user review: â€˜Ramps are well maintained.â€™â€  \n",
    "> - [Wabash Suites]: â€œRoll-in shower, wide hallways, recent guest: â€˜No stairs, great for my wheelchair!â€™â€  \n",
    ">\n",
    "> **Nearby Accessible Restaurants:**  \n",
    "> - [Fresh & Easy Diner]: â€œNo steps, accessible bathroom, all employees are trained.â€  \n",
    "> - [Lakeside Grill]: â€œAccessible by public transit, quiet, braille menus.â€\n",
    ">\n",
    "> **Suggested message to hotel:**  \n",
    "> - â€œHello, can you confirm your entrance and elevator accommodate wheelchairs? Are there accessible bathrooms in the guest rooms?â€\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to demo, extend, and impress!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Code Assistant: In-Notebook and Streamlit Chat\n",
    "\n",
    "This section enables a code assistant using Databricks LLMs. You can ask coding/data questions and get answers or code suggestions directly in the notebook or via the Streamlit UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_databricks import ChatDatabricks\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import streamlit as st\n",
    "\n",
    "# Instantiate a chat LLM for code assistance\n",
    "code_llm = ChatDatabricks(endpoint=\"databricks-llama-4-maverick\", max_tokens=1024)\n",
    "\n",
    "# Define a prompt template for code Q&A\n",
    "code_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful Databricks code assistant. Answer the user's coding or data question. If code is needed, provide a concise, working code snippet for Databricks (PySpark, SQL, or Python as appropriate). If the question is about data access, reference Unity Catalog/Delta best practices. If the question is about RAG or agents, reference Databricks Vector Search and LangChain integration.\n",
    "    \n",
    "    User question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "code_chain = code_prompt | code_llm | StrOutputParser()\n",
    "\n",
    "def code_assistant_ask(question):\n",
    "    \"\"\"Ask the code assistant a question and get an answer/code snippet.\"\"\"\n",
    "    return code_chain.invoke({\"question\": question})\n",
    "\n",
    "# Streamlit UI for code assistant\n",
    "if 'code_chat_history' not in st.session_state:\n",
    "    st.session_state['code_chat_history'] = []\n",
    "\n",
    "with st.expander(\"ðŸ’¬ Code Assistant Chat (Databricks)\"):\n",
    "    st.markdown(\"Ask any Databricks coding/data question. Get code or best practices instantly!\")\n",
    "    code_user_q = st.text_input(\"Your coding/data question\", key=\"code_assistant_input\")\n",
    "    if st.button(\"Ask Code Assistant\"):\n",
    "        if code_user_q:\n",
    "            code_answer = code_assistant_ask(code_user_q)\n",
    "            st.session_state['code_chat_history'].append((code_user_q, code_answer))\n",
    "    for q, a in st.session_state['code_chat_history']:\n",
    "        st.markdown(f\"**You:** {q}\")\n",
    "        st.markdown(f\"**Assistant:** {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "- In notebook: Call `code_assistant_ask(\"your question\")` in a code cell to get a code/data answer.\n",
    "- In Streamlit: Use the \"Code Assistant Chat\" expander to ask questions and see answers interactively.\n",
    "- Example: `code_assistant_ask(\"How do I join two Delta tables in PySpark?\")`\n",
    "- The assistant will return a concise answer or code snippet, referencing Databricks best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Code Assistant with Function Calling (Databricks Agent)\n",
    "\n",
    "This section demonstrates how to enable function calling with the code assistant, so the LLM can trigger Python functions (e.g., data access, RAG, or agent tools) based on user intent. This is inspired by Databricks agent and LangChain function-calling patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_databricks import ChatDatabricks\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define tools for function calling\n",
    "@tool\n",
    "def get_lodging(city: str, need: str = None) -> str:\n",
    "    \"\"\"Find accessible lodging in a city, optionally filtered by need.\"\"\"\n",
    "    df = find_accessible_lodging(city, need)\n",
    "    if df.empty:\n",
    "        return \"No accessible lodging found.\"\n",
    "    return df.to_markdown(index=False)\n",
    "\n",
    "@tool\n",
    "def get_businesses(city: str, need: str = None) -> str:\n",
    "    \"\"\"Find accessible businesses in a city, optionally filtered by need.\"\"\"\n",
    "    df = find_accessible_businesses(city, need)\n",
    "    if df.empty:\n",
    "        return \"No accessible businesses found.\"\n",
    "    return df.to_markdown(index=False)\n",
    "\n",
    "@tool\n",
    "def summarize_reviews(reviews: list, need: str = None) -> str:\n",
    "    \"\"\"Summarize reviews for accessibility features.\"\"\"\n",
    "    return summarize_reviews_llm(reviews, need)\n",
    "\n",
    "@tool\n",
    "def draft_message(place_name: str, need: str) -> str:\n",
    "    \"\"\"Draft an inquiry message for accessibility needs.\"\"\"\n",
    "    return draft_inquiry_message(place_name, need)\n",
    "\n",
    "# List of tools for the agent\n",
    "agent_tools = [get_lodging, get_businesses, summarize_reviews, draft_message]\n",
    "\n",
    "# Set up the agent LLM with function calling\n",
    "agent_llm = ChatDatabricks(endpoint=\"databricks-llama-4-maverick\", max_tokens=1024)\n",
    "\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Databricks travel agent assistant. You can call Python functions to answer user questions about accessible travel, lodging, businesses, and reviews.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "\n",
    "agent = create_openai_functions_agent(\n",
    "    llm=agent_llm,\n",
    "    tools=agent_tools,\n",
    "    prompt=agent_prompt\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=agent_tools, verbose=True)\n",
    "\n",
    "def travel_agent_chat(user_input, history=None):\n",
    "    \"\"\"Chat with the travel agent assistant with function calling.\"\"\"\n",
    "    if history is None:\n",
    "        history = []\n",
    "    messages = [HumanMessage(content=user_input)]\n",
    "    for h in history:\n",
    "        messages.append(AIMessage(content=h))\n",
    "    result = agent_executor.invoke({\"input\": user_input, \"history\": messages})\n",
    "    return result[\"output\"]\n",
    "\n",
    "# Streamlit UI for function-calling agent\n",
    "if 'agent_chat_history' not in st.session_state:\n",
    "    st.session_state['agent_chat_history'] = []\n",
    "\n",
    "with st.expander(\"ðŸ¤– Travel Agent Chat (Function Calling)\"):\n",
    "    st.markdown(\"Ask anything about accessible travel. The agent can call functions to get real data!\")\n",
    "    agent_user_q = st.text_input(\"Your travel question\", key=\"agent_function_input\")\n",
    "    if st.button(\"Ask Travel Agent\", key=\"agent_function_btn\"):\n",
    "        if agent_user_q:\n",
    "            agent_answer = travel_agent_chat(agent_user_q, [a for _, a in st.session_state['agent_chat_history']])\n",
    "            st.session_state['agent_chat_history'].append((agent_user_q, agent_answer))\n",
    "    for q, a in st.session_state['agent_chat_history']:\n",
    "        st.markdown(f\"**You:** {q}\")\n",
    "        st.markdown(f\"**Agent:** {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "- In notebook: Call `travel_agent_chat(\"your question\")` to interact with the function-calling agent.\n",
    "- In Streamlit: Use the \"Travel Agent Chat (Function Calling)\" expander to ask questions and see answers with real function calls.\n",
    "- The agent will call Python functions for data, RAG, or message drafting as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Unified Databricks Agent Orchestrator: Data, Tools, and Travel Planning\n",
    "\n",
    "This section brings together all data access, RAG, SQL, and LLM tools into a single orchestrator agent. The agent can:\n",
    "- Search and join across multiple Delta tables (Airbnb, Booking.com, Google Maps Businesses)\n",
    "- Use RAG/vector search for unstructured/review data\n",
    "- Summarize, draft messages, and plan a full trip\n",
    "- Chain multiple tool calls to fulfill a travel plan (end-to-end agentic workflow)\n",
    "\n",
    "Patterns are inspired by Databricks agent, nimble-mcp, and partner_data_quickstart notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_databricks import ChatDatabricks\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "import pandas as pd\n",
    "\n",
    "# --- Data Access Tools ---\n",
    "@tool\n",
    "def get_airbnb(city: str, need: str = None) -> str:\n",
    "    \"\"\"Query Airbnb Delta table for accessible listings in a city.\"\"\"\n",
    "    if USE_DEMO_DATA:\n",
    "        df = lodging_df[lodging_df[\"location\"].str.contains(city, case=False)]\n",
    "        if need:\n",
    "            mask = (\n",
    "                df[\"amenities\"].str.contains(need, case=False, na=False) |\n",
    "                df[\"description\"].str.contains(need, case=False, na=False) |\n",
    "                df[\"reviews\"].apply(lambda reviews: any(need.lower() in r.lower() for r in reviews))\n",
    "            )\n",
    "            df = df[mask]\n",
    "        return df.to_markdown(index=False) if not df.empty else \"No results.\"\n",
    "    else:\n",
    "        # Example: spark.read.table(\"main.bright_airbnb.listings\") ...\n",
    "        return \"[Delta table query code here]\"\n",
    "\n",
    "@tool\n",
    "def get_booking(city: str, need: str = None) -> str:\n",
    "    \"\"\"Query Booking.com Delta table for accessible listings in a city.\"\"\"\n",
    "    return \"[Delta table query code here]\"\n",
    "\n",
    "@tool\n",
    "def get_gmaps_businesses(city: str, need: str = None) -> str:\n",
    "    \"\"\"Query Google Maps Businesses Delta table for accessible businesses in a city.\"\"\"\n",
    "    if USE_DEMO_DATA:\n",
    "        df = business_df[business_df[\"location\"].str.contains(city, case=False)]\n",
    "        if need:\n",
    "            mask = (\n",
    "                df[\"accessibility\"].str.contains(need, case=False, na=False) |\n",
    "                df[\"reviews\"].apply(lambda reviews: any(need.lower() in r.lower() for r in reviews))\n",
    "            )\n",
    "            df = df[mask]\n",
    "        return df.to_markdown(index=False) if not df.empty else \"No results.\"\n",
    "    else:\n",
    "        return \"[Delta table query code here]\"\n",
    "\n",
    "# --- RAG/Vector Search Tool ---\n",
    "@tool\n",
    "def rag_search(query: str) -> str:\n",
    "    \"\"\"Use Databricks Vector Search to retrieve relevant unstructured info (e.g., reviews, amenities).\"\"\"\n",
    "    rag_results = index.similarity_search(\n",
    "        columns=[\"name\", \"description\", \"amenities\"],\n",
    "        query_text=query,\n",
    "        num_results=3\n",
    "    )\n",
    "    if rag_results and rag_results.get('result') and rag_results['result'].get('data_array'):\n",
    "        df = pd.DataFrame(rag_results['result']['data_array'], columns=[\"name\", \"description\", \"amenities\"])\n",
    "        return df.to_markdown(index=False)\n",
    "    return \"No relevant results.\"\n",
    "\n",
    "# --- LLM/Planning Tools ---\n",
    "@tool\n",
    "def summarize_reviews_tool(reviews: list, need: str = None) -> str:\n",
    "    \"\"\"Summarize reviews for accessibility features.\"\"\"\n",
    "    return summarize_reviews_llm(reviews, need)\n",
    "\n",
    "@tool\n",
    "def draft_inquiry_tool(place_name: str, need: str) -> str:\n",
    "    \"\"\"Draft an inquiry message for accessibility needs.\"\"\"\n",
    "    return draft_inquiry_message(place_name, need)\n",
    "\n",
    "@tool\n",
    "def plan_trip(city: str, need: str = None) -> str:\n",
    "    \"\"\"Plan a full accessible trip: find stays, businesses, summarize, and draft a message.\"\"\"\n",
    "    stays = get_airbnb(city, need)\n",
    "    businesses = get_gmaps_businesses(city, need)\n",
    "    plan = f\"# Accessible Trip Plan for {city}\\n\\n## Stays\\n{stays}\\n\\n## Businesses\\n{businesses}\\n\"\n",
    "    if stays != \"No results.\":\n",
    "        plan += f\"\\n## Suggested Inquiry Message\\n{draft_inquiry_message(city, need)}\\n\"\n",
    "    return plan\n",
    "\n",
    "# --- Orchestrator Agent ---\n",
    "orchestrator_tools = [get_airbnb, get_booking, get_gmaps_businesses, rag_search, summarize_reviews_tool, draft_inquiry_tool, plan_trip]\n",
    "\n",
    "orchestrator_llm = ChatDatabricks(endpoint=\"databricks-llama-4-maverick\", max_tokens=2048)\n",
    "\n",
    "orchestrator_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Databricks travel agent orchestrator. You can call Python functions to search Delta tables, use RAG, summarize, and plan a full accessible trip. Chain tools as needed to fulfill the user's travel planning request.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "orchestrator_agent = create_openai_functions_agent(\n",
    "    llm=orchestrator_llm,\n",
    "    tools=orchestrator_tools,\n",
    "    prompt=orchestrator_prompt\n",
    ")\n",
    "\n",
    "orchestrator_executor = AgentExecutor(agent=orchestrator_agent, tools=orchestrator_tools, verbose=True)\n",
    "\n",
    "def orchestrator_chat(user_input, history=None):\n",
    "    \"\"\"Chat with the orchestrator agent for full travel planning.\"\"\"\n",
    "    if history is None:\n",
    "        history = []\n",
    "    messages = [HumanMessage(content=user_input)]\n",
    "    for h in history:\n",
    "        messages.append(AIMessage(content=h))\n",
    "    result = orchestrator_executor.invoke({\"input\": user_input, \"history\": messages})\n",
    "    return result[\"output\"]\n",
    "\n",
    "# Streamlit UI for orchestrator agent\n",
    "if 'orch_chat_history' not in st.session_state:\n",
    "    st.session_state['orch_chat_history'] = []\n",
    "\n",
    "with st.expander(\"ðŸ§­ Unified Travel Agent Orchestrator (Full Planning)\"):\n",
    "    st.markdown(\"Ask for a full travel plan or any complex travel question. The agent will orchestrate all tools and data sources!\")\n",
    "    orch_user_q = st.text_input(\"Your travel planning request\", key=\"orch_input\")\n",
    "    if st.button(\"Ask Orchestrator Agent\", key=\"orch_btn\"):\n",
    "        if orch_user_q:\n",
    "            orch_answer = orchestrator_chat(orch_user_q, [a for _, a in st.session_state['orch_chat_history']])\n",
    "            st.session_state['orch_chat_history'].append((orch_user_q, orch_answer))\n",
    "    for q, a in st.session_state['orch_chat_history']:\n",
    "        st.markdown(f\"**You:** {q}\")\n",
    "        st.markdown(f\"**Orchestrator:** {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "- In notebook: Call `orchestrator_chat(\"your travel planning request\")` for a full agentic workflow.\n",
    "- In Streamlit: Use the \"Unified Travel Agent Orchestrator\" expander for end-to-end planning and tool chaining.\n",
    "- The agent will search/join data, use RAG, summarize, and plan a trip as needed."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "vector_search_fm_api",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
